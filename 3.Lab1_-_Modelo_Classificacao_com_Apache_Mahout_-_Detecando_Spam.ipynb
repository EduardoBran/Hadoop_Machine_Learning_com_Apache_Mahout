{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c387db",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Lab1: Modelo de Classificação</center></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Contexto\n",
    "\n",
    "Neste laboratório, foi fornecido o arquivo **`NaiveBayes.zip`**, contendo o script `NaiveBayes.sh` e duas pastas: `ham` e `spam`.\n",
    "\n",
    "- A pasta `ham` contém 10 arquivos `.txt` com mensagens consideradas válidas (não spam).\n",
    "- A pasta `spam` contém 10 arquivos `.txt` com mensagens consideradas spam.\n",
    "\n",
    "O arquivo `NaiveBayes.sh` inclui os comandos necessários para a execução do laboratório.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## Problema de Negócio\n",
    "\n",
    "> Detectar se uma mensagem é Spam ou não. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <center>Iniciando o Laboratório</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1 Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **1.3 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## 2. Gravando os Dados no Cluster HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, vamos copiar os arquivos do sistema local para o `HDFS` para que possam ser processados pelo `Mahout`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1 Criando Diretórios no HDFS\n",
    "\n",
    "No terminal digitar os comandos:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -mkdir /mahout\n",
    "hdfs dfs -mkdir /mahout/input\n",
    "hdfs dfs -mkdir /mahout/input/ham\n",
    "hdfs dfs -mkdir /mahout/input/spam\n",
    "```\n",
    "\n",
    "#### 2.2 Lista os arquivos e diretórios no HDFS raiz.\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "#### 2.3 Copiando dados do sistema local para o HDFS\n",
    "\n",
    "No diretório onde estão as pastas `ham` e `spam` ir ao terminal e digitar:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -copyFromLocal ham/* /mahout/input/ham\n",
    "hdfs dfs -copyFromLocal spam/* /mahout/input/spam\n",
    "```\n",
    "\n",
    "#### 2.4 Verificar conteúdo\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /mahout/input/ham\n",
    "hdfs dfs -ls /mahout/input/spam\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## 3. Pré-Processamento dos Dados com Apache Mahout\n",
    "\n",
    "> **Importante**: O pré-processamento a seguir é específico para dados em formato de texto, como o conjunto de dados atual, que contém mensagens armazenadas como arquivos `.txt`. Dependendo do tipo de dados (imagens, dados numéricos, etc.), outras técnicas de pré-processamento podem ser necessárias.\n",
    "\n",
    "> O **objetivo do pré-processamento** é converter os textos em um formato que o Mahout possa utilizar para análise e modelagem.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1 Converter Dados para Formato de Sequence (obrigatório quando se trabalha com Mahout)\n",
    "\n",
    "O primeiro passo no `Mahout` é transformar os dados de texto em uma **sequence file**, um formato binário que facilita o processamento distribuído no `Hadoop`.\n",
    "\n",
    "```bash\n",
    "mahout seqdirectory -i /mahout/input -o /mahout/output/seqoutput\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2 Criar a Matriz Esparsa com Vetores TF-IDF\n",
    "\n",
    "Com os dados em formato de sequence, o próximo passo é converter esses arquivos para uma matriz esparsa de vetores **TF-IDF** (Term Frequency-Inverse Document Frequency). Essa matriz representa os termos em cada documento e permite que o Mahout aplique algoritmos de aprendizado de máquina nos dados.\n",
    "\n",
    "```bash\n",
    "mahout seq2sparse -i /mahout/output/seqoutput -o /mahout/output/sparseoutput\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2 Verificar Saída da Matriz Esparsa\n",
    "\n",
    "Após a criação da matriz esparsa, verifique o diretório de saída para confirmar que os vetores **TF-IDF** foram gerados corretamente:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /mahout/output/sparseoutput\n",
    "```\n",
    "<br>\n",
    "\n",
    "A saída mostra os arquivos e diretórios gerados para representar os dados de texto em formato de vetores esparsos:\n",
    "\n",
    "- **df-count**: Contagem de documentos por termo (usado para calcular **IDF**).\n",
    "- **dictionary.file-0**: Mapeamento de termos para índices únicos.\n",
    "- **frequency.file-0**: Frequência de termos no conjunto de dados.\n",
    "- **tf-vectors**: Vetores de frequência dos termos (**TF**) por documento.\n",
    "- **tfidf-vectors**: Vetores **TF-IDF**, que indicam a relevância de cada termo.\n",
    "- **tokenized-documents**: Documentos tokenizados em palavras.\n",
    "- **wordcount**: Contagem total de palavras. (`hadoop fs -text /mahout/output/sparseoutput/wordcount/*`)\n",
    "\n",
    "<br>\n",
    "\n",
    "Esses dados indicam que o pré-processamento está completo e pronto para uso em modelos de aprendizado de máquina.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## 4. Divisão dos Dados em Treino e Teste\n",
    "\n",
    "<br>\n",
    "\n",
    "A divisão dos dados em conjuntos de treino e teste é essencial para avaliar a performance do modelo em dados novos. O Mahout permite fazer essa divisão automaticamente a partir do conjunto de vetores **TF-IDF** criado anteriormente. Nesta etapa, selecionamos 30% dos dados para o conjunto de teste, e os 70% restantes serão usados para treinar o modelo.\n",
    "\n",
    "\n",
    "```bash\n",
    "# Explicação de cada parâmetro\n",
    "#\t-i\t                    pasta com dados de entrada\n",
    "#\t--trainingOutput\t    dados de treino\n",
    "#\t--testOutput\t\t    dados de teste\n",
    "#\t--randomSelectionPct\tpercentual de divisão dos dados\n",
    "#\t--overwrite\t\t\t    overwrite\n",
    "#\t--sequenceFiles\t\t    input sequencial\n",
    "#\t--xm\t\t\t\t    tipo de processamento. \n",
    "\n",
    "mahout split -i /mahout/output/sparseoutput/tfidf-vectors --trainingOutput /mahout/nbTrain --testOutput /mahout/nbTest --randomSelectionPct 30 --overwrite --sequenceFiles -xm sequencial\n",
    "\n",
    "\n",
    "# Verifique o diretório de treino\n",
    "hdfs dfs -ls /mahout/nbTrain\n",
    "\n",
    "# Verifique o diretório de teste\n",
    "hdfs dfs -ls /mahout/nbTest\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## 5. Treinando o Modelo com Dados no Cluster HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora, com os dados de treino devidamente preparados, é hora de aplicar o algoritmo **Naive Bayes** usando o **Apache Mahout** ao conjunto de dados, gerando um modelo que pode classificar mensagens como **spam** ou **ham** (não spam). O modelo treinado será armazenado no `HDFS` para uso posterior nas etapas de teste e avaliação.\n",
    "\n",
    "```bash\n",
    "# Explicação de cada parâmetro\n",
    "#\t-i\tdados de treino\n",
    "#\t-li onde armazenar os labels\n",
    "#\t-o\tonde armazenar o modelo\n",
    "#\t-ow\toverwrite\n",
    "#\t-c\tcomplementary\n",
    "mahout trainnb -i /mahout/nbTrain -li /mahout/nbLabels -o /mahout/nbmodel -ow -c\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## 6. Testando e Avaliando o Modelo\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora, com o modelo treinado, o próximo passo é testá-lo nos dados de teste para verificar sua precisão e eficácia. O `Mahout` utilizará o modelo salvo em `/mahout/nbmodel` para fazer previsões sobre as mensagens no conjunto de teste. O comando `testnb` gera previsões para cada entrada do conjunto de teste e salva os resultados.\n",
    "\n",
    "```bash\n",
    "# Teste do Modelo\n",
    "#\t-i  pasta com os dados de teste\n",
    "#\t-m\tpasta do modelo\n",
    "#\t-l\tlabels \n",
    "#\t-ow\toverwrite\n",
    "#\t-o\tpasta com as previsões\n",
    "#\t-c\tcomplementary \n",
    "mahout testnb -i /mahout/nbTest -m /mahout/nbmodel -l /mahout/nbLabels -ow -o /mahout/nbpredictions -c\n",
    "```\n",
    "<br>\n",
    "\n",
    "A saída contém:\n",
    "\n",
    "- **nbpredictions**: Contém as previsões realizadas pelo modelo, indicando se cada mensagem foi classificada como spam ou ham.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Fim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adedda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
