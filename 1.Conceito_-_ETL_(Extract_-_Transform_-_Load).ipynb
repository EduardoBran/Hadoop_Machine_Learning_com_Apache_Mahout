{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add3d876",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Conceito Sobre <u>ETL</u> (*Extract* - *Transform* - *Load*)</center></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "Em qualquer projeto de **Análise de Dados**, seja com **Big Data** ou não, uma tarefa é sempre essencial: o **ETL**.\n",
    "\n",
    "As **Operações de ETL** (*Extract*, *Transform*, *Load*) são um processo de **extração de dados** de uma fonte, seguido pela **transformação** dos dados para atender a requisitos específicos e, finalmente, o **carregamento** dos dados em um repositório. Esse repositório é então utilizado para o **processo de análise**.\n",
    "\n",
    "O **ETL** é especialmente comum em ambientes de *BI* (*Business Intelligence*), sendo amplamente utilizado para a criação de **Data Marts** (conjuntos de dados específicos para uma área de negócios, como vendas ou marketing, dentro de uma organização) e **Data Warehouses** (repositórios de dados centralizados e estruturados para toda a organização, que integram informações de múltiplas fontes para análise e tomada de decisões estratégicas).\n",
    "\n",
    "O **Hadoop** possui duas ferramentas principais em seu ecossistema:\n",
    " - `Apache Sqoop`: Normalmente usado para **carga de dados em batch** (em lotes), transferindo grandes volumes de dados de bancos de dados relacionais para o Hadoop, facilitando a movimentação de dados estruturados.\n",
    " - `Apache Flume`: Ideal para a **ingestão de dados em tempo real, especialmente dados de log**. O `Flume` é amplamente utilizado para coletar e agregar dados de fontes contínuas e não estruturadas, como logs de servidores e eventos em tempo real.\n",
    " \n",
    "O **Apache Sqoop** será estudado neste capítulo.\n",
    " \n",
    "<br><br>\n",
    "\n",
    "### O que é ETL (*Extract* - *Transform* - *Load*) ?\n",
    "\n",
    "<br>\n",
    "\n",
    "O **ETL** é um processo de integração de dados que envolve três etapas principais: **extração**, **transformação** e **carga** dos dados. Ele permite mover dados de uma ou várias fontes para um destino, onde esses dados serão analisados.\n",
    "\n",
    "- **Extract (Extrair)**: Nessa etapa, os dados são obtidos de suas fontes, que podem variar amplamente, como bancos de dados, APIs, ou arquivos. Cada fonte pode exigir um **método específico de extração** (login com usuário e senha, comandos específicos, ou chamada de APIs), de acordo com suas características.\n",
    "\n",
    "- **Transform (Transformar)**: Esta etapa garante que os dados estejam consistentes e prontos para uso. Transformar é importante porque os dados geralmente vêm desorganizados ou em diferentes formatos. Por exemplo, dados de um formulário web podem estar com formatos variados devido à ausência de validação no preenchimento. A transformação inclui **limpar**, **padronizar** e **estruturar** os dados, preparando-os para a análise.\n",
    "\n",
    "- **Load (Carregar)**: Aqui, os dados já transformados são carregados no sistema de destino, como um **data warehouse** ou **data lake**. Este é o passo final que disponibiliza os dados prontos para consulta e análise.\n",
    "\n",
    "<br>\n",
    "\n",
    "O **ETL** é fundamental para preparar dados de forma consistente e confiável para **Business Intelligence** e **Análise de Dados**, integrando informações de múltiplas fontes em um único repositório para insights e tomada de decisões.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Principais Ferramentas ETL do Mercado\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Principais Ferramentas Propietárias (Pagas)\n",
    "\n",
    "- Informatica Power Center\n",
    "- IBM InfoSphere Data Stage\n",
    "- Oracle Data Integrator (ODI)\n",
    "- Microsoft - SQL Server Integration Services (SSIS)\n",
    "- SAS - Data Integration Studio\n",
    "- SAP - Business Object Integrator\n",
    "- Pentaho Data Integration\n",
    "\n",
    "---\n",
    "\n",
    "#### Principais Ferramentas Open Source (Gratuitas)\n",
    "\n",
    "- Dataiku Data Science Studio (DSS) *Community Edition*\n",
    "- Talend Open Studio For Data Integration\n",
    "- Jaspersoft ETL\n",
    "- Jedox\n",
    "- RapidMiner\n",
    "- Apache NiFi\n",
    "- Apache Flume\n",
    "- Apache Sqoop\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Passo a Passo para a Instalação Banco Oracle\n",
    "\n",
    "<br>\n",
    "\n",
    "> Verifcar o arquivo **Oracle-Installation.txt**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c895f8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u><span style=\"font-size: 34px;color: darkgreen;\">Lab - Carregando 20 milhões de Registros no Banco de Dados Oracle </span></center></u>\n",
    "\n",
    "<br>\n",
    "\n",
    "Neste laboratório, iremos **carregar 20 milhões de registros** em uma tabela no `Banco de Dados Oracle`. Em seguida, utilizaremos o `Apache Sqoop` como ferramenta **ETL** para transferir **uma amostra desses dados** do `Banco Oracle` para o `HDFS`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o Listener digitando no terminal**:\n",
    "   ```java\n",
    "   lsnrctl start  |  lsnrctl stop\n",
    "   ```\n",
    "- **1.2 Iniciar o Banco de Dados digitando no terminal**:\n",
    "   ```java\n",
    "   sqlplus / as sysdba\n",
    "   ```\n",
    "- **1.3 Dentro do terminal do Oracle, digitar**:\n",
    "   ```java\n",
    "   startup  |  shutdown immediate\n",
    "   ```\n",
    "- **1.4 Verificar Status**:\n",
    "   ```java\n",
    "   lsnrctl status\n",
    "   ```\n",
    "   \n",
    "<br><br><br>\n",
    "\n",
    "## 2. Carregando Dados Para o Banco Oracle\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, utilizaremos o **`SQL*Loader`** (**etapa 2.5.4**), uma ferramenta da `Oracle` projetada para carregar grandes volumes de dados de maneira rápida e eficiente em tabelas de um banco de dados Oracle. Ele é especialmente útil para transferir dados de arquivos externos (como arquivos CSV ou de texto) diretamente para o banco, simplificando o processo de `ETL` (Extração, Transformação e Carga) no Oracle.\n",
    "\n",
    "Antes de carregar os dados, precisamos criar um **usuário**, que, no `Oracle`, também equivale a criar um **schema**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**O que é um `schema`?**\n",
    "\n",
    "No `Oracle`, o **schema** é automaticamente associado a um **usuário** e serve como uma **área específica de armazenamento dentro do banco de dados**. Ele organiza e isola as tabelas, dados e outros objetos (como *índices* e *views*) pertencentes a esse *usuário*. Assim, cada **usuário tem seu próprio schema**, o que mantém o banco de dados organizado e seguro ao separar e proteger os dados por diferentes proprietários.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 2.1 Criando Um Schema\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.1.1 Acessar Terminal do sqlplus**:\n",
    "   ```java\n",
    "   sqlplus / as sysdba\n",
    "   ```\n",
    "- **2.1.2  Criando o schema (usuário) `aluno`**:\n",
    "   ```java\n",
    "   create user aluno identified by digitar_senha;\n",
    "   ```\n",
    "- **2.1.3 Conceder previlégios necessários para `aluno`**:\n",
    "   ```java\n",
    "   grant connect, resource, unlimited tablespace to aluno;\n",
    "   ```\n",
    "- **2.1.4 Sair do terminal do sqlplus**:\n",
    "   ```java\n",
    "   exit\n",
    "   ```\n",
    "   \n",
    "<br><br>\n",
    "\n",
    "### 2.2 Conectando com o Schema `aluno`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.2.1 Acessar Terminal do sqlplus como `aluno` (digitar senha)**:\n",
    "   ```bash\n",
    "   sqlplus aluno@orcl\n",
    "   ```\n",
    "   \n",
    "<br><br>\n",
    "\n",
    "### 2.3 Criando uma Tabela\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.3.1 Acessar Terminal do sqlplus como `aluno` e digitar**:\n",
    "   ```bash\n",
    "   CREATE TABLE cinema ( \n",
    "     ID   NUMBER PRIMARY KEY , \n",
    "     USER_ID       VARCHAR2(30), \n",
    "     MOVIE_ID      VARCHAR2(30), \n",
    "     RATING        DECIMAL, \n",
    "     TIMESTAMP     VARCHAR2(256) );\n",
    "   ```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 2.4 Realizando Download dos Dados Para Carregar na Tabela\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.4.1 Acessar o link e realizar o dowload**:\n",
    "   ```bash\n",
    "   http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Importante**: O dataset `ratings.csv` baixado anteriormente contém as seguintes colunas `ID`, `USER_ID`, `MOVIE_ID`, `RATING` e `TIMESTAMP`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 2.5 Preparando a Carga de Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.5.1 Navegar até diretório *home* criar um novo diretório chamado `etl`**:\n",
    "   ```bash\n",
    "   mkdir etl\n",
    "   ```\n",
    "- **2.5.2 Dentro do diretório `etl` criar o arquivo `loader.dat` digitando no terminal**:\n",
    "   ```bash\n",
    "   gedit loader.dat\n",
    "   ```\n",
    "- **2.5.3 No arquivo `loader.dat` colar o conteúdo e salvar**:\n",
    "   ```bash\n",
    "load data\n",
    "INFILE '/home/oracle/Downloads/ml-20m/ratings.csv'\n",
    "INTO TABLE cinema\n",
    "APPEND\n",
    "FIELDS TERMINATED BY ','\n",
    "trailing nullcols\n",
    "(id SEQUENCE (MAX,1),\n",
    " user_id CHAR(30),\n",
    " movie_id CHAR(30),\n",
    " rating   decimal external,\n",
    " timestamp  char(256))\n",
    "   ```\n",
    "- **2.5.4 Executando o arquivo `loader.dat` no terminal do diretório `etl`**:\n",
    "   ```bash\n",
    "   sqlldr userid=aluno/digitar_senha control=loader.dat log=loader.log\n",
    "   ```\n",
    "- **2.5.5 Verificar Arquivo de Log**:\n",
    "   ```bash\n",
    "   gedit loader.log\n",
    "   ```\n",
    "<br><br>\n",
    "\n",
    "### 2.6 Verificando Dados Carregados no Banco Oracle\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.6.1 Acessar Terminal do sqlplus como `aluno` (digitar senha) ou outro schema/usuário**:\n",
    "   ```bash\n",
    "   sqlplus aluno@orcl\n",
    "   ```\n",
    "- **2.6.2 Verificando tabelas no schema `aluno`**:\n",
    "   ```bash\n",
    "   select table_name FROM user_tables;\n",
    "   ```\n",
    "- **2.6.3 Visualizando *count* (*quantidade*) de linhas da tabela `cinema`**:\n",
    "   ```bash\n",
    "   select count(*) from cinema;\n",
    "   ```\n",
    "- **2.6.4 Visualizando as 5 primeiras linhas da tabela `cinema`**:\n",
    "   ```bash\n",
    "   SELECT * FROM cinema WHERE ROWNUM <= 5;\n",
    "   ```   \n",
    "- **2.6.5 Visualizando 5 linhas aleatórias da tabela `cinema`**:\n",
    "   ```bash\n",
    "   SELECT * FROM cinema ORDER BY dbms_random.value FETCH FIRST 5 ROWS ONLY;\n",
    "   ```\n",
    "- **2.6.6 Ajustando a exibição temporária para melhor visualizar**:\n",
    "   ```bash\n",
    "   SET LINESIZE 100\n",
    "   SET PAGESIZE 20\n",
    "   ```  \n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 3. Configurando o Sistema Para Importação de Dados do Banco de Dados Oracle para o HDFS com o Apache Sqoop\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Sua empresa possui **milhões de registros de avaliações de filmes** (dados gerados na etapa ***2.***) e deseja utilizá-los para criar um **sistema de recomendação de filmes** para seus clientes.\n",
    "\n",
    "Esses dados estão armazenados em um **banco de dados relacional** (`Oracle`), enquanto a empresa conta com um cluster **Hadoop** para armazenamento e processamento distribuídos.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Transferir os dados da **fonte** para o **HDFS** a fim de realizar análises e desenvolver o sistema de recomendação.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Apache Sqoop\n",
    "\n",
    "Usaremos como **ferramenta ETL** o `Apache Sqoop` que é um dos componentes do ecossistema do `Hadoop`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3.1 Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "####  <u>Hadoop</u>\n",
    "\n",
    "> **<u>Importante</u>**: é necessário está logado com o usuário **hadoop** para inicializar os **serviços do hadoop**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **3.1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **3.1.3 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### <u>Oracle</u>\n",
    "\n",
    "> **<u>Importante</u>**: é necessário está logado com o usuário **oracle** para inicializar o **banco oracle**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.1.4 Iniciar o Listener digitando no terminal**:\n",
    "   ```java\n",
    "   lsnrctl start  |  lsnrctl stop\n",
    "   ```\n",
    "- **3.1.5 Iniciar o Banco de Dados digitando no terminal**:\n",
    "   ```java\n",
    "   sqlplus / as sysdba\n",
    "   ```\n",
    "- **3.1.6 Dentro do terminal do Oracle, digitar**:\n",
    "   ```java\n",
    "   startup  |  shutdown immediate  |  exit\n",
    "   ```\n",
    "- **3.1.7 Verificar Status**:\n",
    "   ```java\n",
    "   lsnrctl status\n",
    "   ```\n",
    "- **3.1.8 Acessar Terminal do sqlplus como `aluno` (digitar senha) ou outro schema/usuário**:\n",
    "   ```bash\n",
    "   sqlplus aluno@orcl\n",
    "   ```\n",
    "- **3.1.9 Verificando tabelas no schema `aluno`**:\n",
    "   ```bash\n",
    "   select table_name FROM user_tables;\n",
    "   ```\n",
    "   \n",
    "<br><br><br>\n",
    "\n",
    "## 3.2 Configurando o Driver JDBC\n",
    "\n",
    "<br>\n",
    "\n",
    "O `Apache Sqoop` conecta-se ao `Banco Oracle` usando o **driver JDBC**.\n",
    "\n",
    "Cada **banco relacional** possui seu próprio **driver JDBC**, como o `MySQL` e o `PostgreSQL`. \n",
    "\n",
    "No caso do` Oracle`, é necessário fazer o **download do driver JDBC** específico diretamente no site da Oracle:\n",
    "\n",
    "https://www.oracle.com/database/technologies/jdbc-ucp-122-downloads.html\n",
    "\n",
    "Após o download, acesse a pasta onde o arquivo foi descompactado, localize o `ojdbc8.jar` e mova-o para o diretório do `Sqoop`, permitindo que o `Sqoop` reconheça o **driver JDBC**. O procedimento para drivers de outros bancos é o mesmo. No terminal digite:\n",
    "\n",
    "- Conectar como usuário root: `su` (depois digite a senha)\n",
    "- Como usuário root copie o arquivo com: `cp ojdbc8.jar /opt/sqoop/lib`\n",
    "- Acessar diretório do sqoop: `cd /opt/sqoop/lib/`\n",
    "- Alterar propriedade do arquivo ojdbc8.jar: `chown hadoop:hadoop ojdbc8.jar`\n",
    "- Sair do usuário root: `exit`\n",
    "\n",
    "<br><br><br>  chown hadoop:hadoop commons-lang-2.6.jar\n",
    "\n",
    "## 3.3 Ajustando os Previlégios de Acesso\n",
    "\n",
    "<br>\n",
    "\n",
    "Em um **Ambiente de Produção** em empresas de médio e grande porte, cada *software* normalmente é executado em um servidor separado. Por exemplo:\n",
    "\n",
    "- Um servidor é dedicado ao `Banco Oracle`,\n",
    "- Outro servidor é dedicado ao `Apache Sqoop`,\n",
    "- E uma máquina ou um grupo de máquinas são dedicadas ao `Apache HDFS`.\n",
    "\n",
    "Neste laboratório, configuramos um **Ambiente de Teste** em uma única máquina. Nessa configuração, todos os serviços estão instalados, mas distribuídos em **diferentes usuários** no mesmo sistema operacional. Essa configuração gera desafios, pois cada serviço possui seu próprio usuário.\n",
    "\n",
    "Dentro deste cenário, o **usuário oracle**, que executa o `Banco Oracle`, atualmente não possui acesso ao `HDFS`, pois ele está instalado no **usuário hadoop**. Para que o `Sqoop` possa acessar o `HDFS` e funcionar corretamente, é necessário **conceder acesso ao usuário oracle** para que ele consiga interagir com o `HDFS`.\n",
    "\n",
    "O **objetivo** é **ajustar os privilégios de acesso** de modo que os diferentes usuários no sistema possam interagir com os serviços necessários, permitindo o uso completo de todos os serviços em uma única máquina de teste.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.3.1 No `usuario oracle`, configurar as variáveis de ambiente para o Hadoop e Sqoop**.\n",
    "  - Ir ao terminal e digitar: `gedit .bashrc`\n",
    "  - No arquivo .bashrch colar o conteúdo:\n",
    "   ```code\n",
    "    # Java JDK\n",
    "    export JAVA_HOME=/opt/jdk\n",
    "    export PATH=$PATH:$JAVA_HOME/bin\n",
    "    \n",
    "    # Hadoop\n",
    "    export HADOOP_HOME=/opt/hadoop\n",
    "    export HADOOP_INSTALL=$HADOOP_HOME\n",
    "    export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "    export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "    export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "    export YARN_HOME=$HADOOP_HOME\n",
    "    export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "    \n",
    "    # Sqoop\n",
    "    export SQOOP_HOME=/opt/sqoop\n",
    "    export PATH=$PATH:$SQOOP_HOME/bin\n",
    "    export HCAT_HOME=/opt/sqoop/hcatalog\n",
    "    export ACCUMULO_HOME=/opt/sqoop/accumulo\n",
    "   ```\n",
    "   \n",
    "   - Após salvar digite: `source .bashrc`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.3.2 No `usuario hadoop`, definir os privilégios com os comandos abaixos**: \n",
    "\n",
    "```code\n",
    "    hdfs dfsadmin -safemode leave\n",
    "    hdfs dfs -chmod -R 777 /\n",
    "    chmod -R 777 /opt/hadoop/logs\n",
    "    exit\n",
    "```\n",
    "<br>\n",
    "\n",
    "- **3.3.3 No `usuario root`, definir os privilégios com os comandos abaixos**:\n",
    "\n",
    "```code\n",
    "    groups oracle\n",
    "    usermod -a -G hadoop oracle\n",
    "    groups oracle\n",
    "    exit\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.3.4 No `usuario oracle`, testar digitando no terminal**:\n",
    "\n",
    "```code\n",
    "    hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 4. Transferindo os Dados do Oracle para o HDFS com o Apache Sqoop\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.1 No `usuario oracle`, acessar o diretório `etl` e digitar no terminal**:\n",
    "\n",
    "```code\n",
    "sqoop import --connect jdbc:oracle:thin:aluno/dsahadoop@dataserver.localdomain:1539/orcl --username aluno -password sua_senha --query \"select user_id, movie_id from cinema where rating = 1 and \\$CONDITIONS\" --target-dir /user/oracle/output -m 1\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.2 Checar se diretório `oracle` foi criado com**:\n",
    "\n",
    "```code\n",
    "hdfs dfs -ls /user\n",
    "hdfs dfs -ls /user/oracle\n",
    "hdfs dfs -ls /user/oracle/output\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.2 Visualizando conteúdo**:\n",
    "\n",
    "```code\n",
    "hdfs dfs -cat /user/oracle/output/part-m-00000\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.3 Verificando quantidade de linhas**:\n",
    "\n",
    "```code\n",
    "hdfs dfs -cat /user/oracle/output/part-m-00000 | wc -l\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.4 No diretório `etl` foi gerado o arquivo `QueryResult.java` que contém o código do MapReduce necessário para importação dos dados**:\n",
    "\n",
    "```code\n",
    "gedit QueryResult.java\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.5 Comando usado para corrigir problemas com blocos corrompidos, caso ocorra**:\n",
    "\n",
    "```code\n",
    "hdfs fsck / | egrep -v '^\\.+$' | grep -v replica | grep -v Replica\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **4.6 Para deixar o modo de segurança do Hadoop caso ocorrar algum problema**:\n",
    "\n",
    "```code\n",
    "hdfs dfsadmin -safemode leave\n",
    "```\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Alternando Entre Usuários\n",
    "\n",
    "- **Conectar como usuário root**: `su` (digitar senha)\n",
    "- **Digitar**: `su - nome_usuario`\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Fim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a51279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
